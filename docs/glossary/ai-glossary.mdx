---
title: "AI glossary"
description: "Copy for LLMView as Markdown"
---

| Term                                 | Definition                                                                              |
| ------------------------------------ | --------------------------------------------------------------------------------------- |
| AI Alignment                         | Ensuring AI behavior aligns with human goals and values.                                |
| API Latency                          | Time delay between request and response in a deployed AI service.                       |
| AUC (Area Under Curve)               | Measure of model’s ability to distinguish between classes.                              |
| Accuracy                             | Proportion of correct predictions made by a model.                                      |
| Activation Function                  | Function that introduces non-linearity into a neural network (e.g., ReLU, Sigmoid).     |
| Artificial Intelligence (AI)         | Simulation of human intelligence in machines programmed to think and learn.             |
| Attention Mechanism                  | Mechanism that enables models to focus on relevant parts of the input.                  |
| Backpropagation                      | Algorithm for updating neural network weights.                                          |
| Batch Normalization                  | Normalizes activations to improve training speed and stability.                         |
| Bias                                 | Systematic error introduced by assumptions in data or model.                            |
| Chain-of-Thought (CoT)               | Technique where models generate intermediate reasoning steps.                           |
| Computer Vision                      | AI field that enables machines to interpret and make decisions based on visual data.    |
| Confusion Matrix                     | Table showing true vs predicted classifications.                                        |
| Data Privacy                         | Protection of sensitive user data during model training and usage.                      |
| Deep Learning                        | Subset of ML using neural networks with many layers to model complex patterns.          |
| Dropout                              | Regularization technique that randomly drops units in a neural network during training. |
| Embedding                            | Numerical representation of data, often used for similarity or search.                  |
| Epoch                                | One complete pass through the training dataset.                                         |
| Explainable AI (XAI)                 | Techniques to interpret and understand model predictions.                               |
| F1 Score                             | Harmonic mean of precision and recall.                                                  |
| Few-shot Learning                    | Model learns from a few labeled examples.                                               |
| Fine-Tuning                          | Training a pre-trained model on a specific task or dataset.                             |
| Generative AI                        | AI models that can generate new content like text, images, audio, or code.              |
| Gradient Descent                     | Optimization algorithm for minimizing loss function.                                    |
| Hallucination                        | Generated output that is fluent but factually incorrect.                                |
| HuggingFace                          | Ecosystem for pretrained NLP models and transformers.                                   |
| Inference                            | Running a trained model to make predictions.                                            |
| LangChain                            | Framework for building LLM-powered applications using composable chains.                |
| Large Language Model (LLM)           | A transformer-based model trained on large corpora of text data.                        |
| LlamaIndex                           | Tool for indexing and querying external data with LLMs.                                 |
| LoRA (Low-Rank Adaptation)           | Efficient method for fine-tuning large models with fewer parameters.                    |
| Loss Function                        | Function that measures error between predicted and actual values.                       |
| Machine Learning (ML)                | Subset of AI that allows systems to learn from data without explicit programming.       |
| Mixture of Experts                   | Model architecture that routes input through subsets of expert networks.                |
| Model Checkpointing                  | Saving model states during training to resume or analyze progress.                      |
| Model Fairness                       | Ensuring model performance does not discriminate against subgroups.                     |
| Model Serving                        | Hosting and providing access to ML models for inference.                                |
| Multimodal AI                        | Models that process and generate across multiple data types (e.g., text, image, audio). |
| Natural Language Processing (NLP)    | AI branch that deals with understanding and generation of human language.               |
| Neural Network                       | Computational model inspired by the human brain, used in deep learning.                 |
| ONNX                                 | Open Neural Network Exchange; format for model interoperability.                        |
| Open Weight Model                    | AI model with publicly available weights for reuse and fine-tuning.                     |
| Overfitting                          | Model learns training data too well and fails to generalize.                            |
| Positional Encoding                  | Adds information about the position of tokens in sequences to transformer models.       |
| Pre-training                         | Initial training of a model on a large generic dataset.                                 |
| Precision                            | Proportion of true positive predictions among all positive predictions.                 |
| Prompt Engineering                   | Crafting effective prompts to guide LLM responses.                                      |
| Proprietary Model                    | Model with restricted access, typically hosted and maintained by a company.             |
| PyTorch                              | Popular deep learning library developed by Facebook.                                    |
| ROC Curve                            | Graphical plot showing performance of classification model.                             |
| Recall                               | Proportion of true positives among all actual positives.                                |
| Reinforcement Learning               | Learning method where agents learn by taking actions and receiving rewards.             |
| Residual Connection                  | Skip connections in neural networks that help prevent vanishing gradients.              |
| Responsible AI                       | Ethical and accountable development and deployment of AI.                               |
| Retrieval-Augmented Generation (RAG) | LLM approach that augments prompts with relevant context from a document store.         |
| Scikit-learn                         | Python library for traditional machine learning.                                        |
| Self-Attention                       | Mechanism allowing models to weigh importance of different parts of input.              |
| Semi-Supervised Learning             | Uses a small amount of labeled data with a large amount of unlabeled data.              |
| Supervised Learning                  | Training a model on labeled data.                                                       |
| TensorFlow                           | Google’s open-source deep learning framework.                                           |
| TensorRT                             | NVIDIA platform for optimizing and deploying deep learning models.                      |
| Tokenization                         | Breaking text into smaller units (tokens) for processing.                               |
| Transformer                          | Deep learning model architecture that uses self-attention for sequence tasks.           |
| Underfitting                         | Model is too simple to capture underlying patterns in data.                             |
| Unsupervised Learning                | Training a model on data without labels to find patterns.                               |
| Variance                             | Model's sensitivity to small fluctuations in training data.                             |
| Vector Database                      | Database optimized for storing and querying high-dimensional embeddings.                |
| Zero-shot Learning                   | Model predicts on tasks without having seen labeled examples.                           |
